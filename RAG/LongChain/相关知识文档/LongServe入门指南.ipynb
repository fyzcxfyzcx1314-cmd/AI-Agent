{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ba908f",
   "metadata": {},
   "source": [
    "# LangServe 入门指南\n",
    "\n",
    "## 一、LangServe 简介\n",
    "\n",
    "### 1.1 什么是 LangServe？\n",
    "\n",
    "LangServe 是一个专为部署 LangChain 应用而设计的高性能服务框架，它能够将 LangChain 构建的各种语言模型应用快速转换为生产就绪的 API 服务。通过 LangServe，你可以轻松地将提示模板、链（Chain）和代理（Agent）等组件部署为 RESTful API，无需编写复杂的服务代码。\n",
    "\n",
    "### 1.2 核心优势\n",
    "\n",
    "- **一键部署**：只需一行代码即可将 LangChain 组件转换为完整的 API 服务\n",
    "- **自动文档**：基于 FastAPI 自动生成交互式 API 文档（Swagger UI/ReDoc）\n",
    "- **类型安全**：利用 Pydantic 和类型提示确保 API 输入输出的安全性\n",
    "- **高性能**：基于 FastAPI 和 Uvicorn，支持异步处理和高并发\n",
    "- **流式支持**：原生支持 LLM 的流式输出\n",
    "- **易于扩展**：完全兼容 FastAPI 生态系统，可以轻松添加自定义路由和中间件\n",
    "\n",
    "### 1.3 应用场景\n",
    "\n",
    "- 将 LangChain 构建的聊天机器人部署为 API 服务\n",
    "- 快速搭建 AI 驱动的工具链服务\n",
    "- 为前端应用提供语言模型后端支持\n",
    "- 构建微服务架构中的 AI 组件\n",
    "\n",
    "## 二、安装与环境准备\n",
    "\n",
    "### 2.1 安装依赖\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "pip install langserve\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 前置要求\n",
    "\n",
    "- Python 3.8+\n",
    "- 熟悉 LangChain 基本概念（提示模板、链、代理等）\n",
    "- 了解 FastAPI 基础知识（可选）\n",
    "\n",
    "## 三、快速开始\n",
    "\n",
    "### 3.1 部署一个简单的链服务\n",
    "\n",
    "以下是一个将简单提示模板部署为服务的完整示例：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langserve import add_routes\n",
    "\n",
    "# 初始化 LangChain 组件\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"请回答以下问题：{question}\"\n",
    ")\n",
    "llm = OpenAI(temperature=0.7)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 创建 FastAPI 应用\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"一个基于 LangChain 的问答服务\",\n",
    ")\n",
    "\n",
    "# 添加 LangChain 链作为路由\n",
    "add_routes(app, chain, path=\"/qa\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 运行服务\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "python main.py\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3.3 访问 API\n",
    "\n",
    "服务启动后，可以通过以下方式访问：\n",
    "\n",
    "#### 标准调用\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:8000/qa/invoke' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"question\": \"什么是机器学习？\"\n",
    "}'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 流式调用\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:8000/qa/stream' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"question\": \"什么是机器学习？\"\n",
    "}'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3.4 查看 API 文档\n",
    "\n",
    "访问以下 URL 查看自动生成的交互式文档：\n",
    "\n",
    "- Swagger UI: <http://localhost:8000/docs>\n",
    "- ReDoc: <http://localhost:8000/redoc>\n",
    "\n",
    "## 四、核心概念与功能\n",
    "\n",
    "### 4.1 主要端点\n",
    "\n",
    "LangServe 为每个部署的链自动创建两个主要端点：\n",
    "\n",
    "1. **POST /invoke**\n",
    "   - 功能：执行链的完整调用\n",
    "   - 请求体：链的输入参数（自动从链的输入模式推导）\n",
    "   - 响应：链的输出结果\n",
    "2. **POST /stream**\n",
    "   - 功能：流式返回链的输出（如果链支持流式输出）\n",
    "   - 请求体：与 `/invoke` 相同\n",
    "   - 响应：流式 JSON 数据\n",
    "\n",
    "### 4.2 输入输出模型\n",
    "\n",
    "LangServe 会根据链的结构自动生成 Pydantic 模型：\n",
    "\n",
    "- **输入模型**：基于链的输入变量\n",
    "- **输出模型**：基于链的输出格式\n",
    "\n",
    "例如，对于前面的问答链，输入模型为：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "class Input(BaseModel):\n",
    "    question: str\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "输出模型为：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "class Output(BaseModel):\n",
    "    text: str\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4.3 自定义输入输出\n",
    "\n",
    "如果你需要更复杂的输入输出结构，可以使用 `@custom_serve` 装饰器：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "from langserve import custom_serve\n",
    "\n",
    "@custom_serve\n",
    "def custom_format_chain(chain):\n",
    "    # 自定义输入模型\n",
    "    class CustomInput(BaseModel):\n",
    "        query: str\n",
    "        context: Optional[str] = None\n",
    "    \n",
    "    # 自定义输出模型\n",
    "    class CustomOutput(BaseModel):\n",
    "        answer: str\n",
    "        sources: List[str]\n",
    "    \n",
    "    return chain.with_types(\n",
    "        input_type=CustomInput,\n",
    "        output_type=CustomOutput,\n",
    "    )\n",
    "\n",
    "# 部署自定义格式的链\n",
    "add_routes(app, custom_format_chain(chain), path=\"/custom-qa\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4.4 部署复杂链\n",
    "\n",
    "LangServe 可以轻松部署包含多个步骤的复杂链：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# 定义多个步骤的链\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"为以下主题生成标题：{topic}\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"title\")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"title\"],\n",
    "    template=\"为以下标题生成内容：{title}\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"content\")\n",
    "\n",
    "# 组合链\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"title\", \"content\"]\n",
    ")\n",
    "\n",
    "# 部署复杂链\n",
    "add_routes(app, sequential_chain, path=\"/content-generator\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 五、高级用法\n",
    "\n",
    "### 5.1 添加认证\n",
    "\n",
    "LangServe 完全兼容 FastAPI 的安全机制：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "from fastapi import Depends, HTTPException\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "\n",
    "security = HTTPBearer()\n",
    "\n",
    "def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    if credentials.scheme != \"Bearer\" or credentials.credentials != \"your-secret-token\":\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid authentication credentials\")\n",
    "    return credentials.credentials\n",
    "\n",
    "# 添加认证依赖\n",
    "add_routes(app, chain, path=\"/secure-qa\", dependencies=[Depends(verify_token)])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 5.2 自定义路由\n",
    "\n",
    "如果你需要添加额外的自定义路由：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "from fastapi import APIRouter\n",
    "\n",
    "# 创建自定义路由\n",
    "router = APIRouter()\n",
    "\n",
    "@router.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "# 将自定义路由添加到应用\n",
    "app.include_router(router, prefix=\"/api\")\n",
    "\n",
    "# 部署 LangChain 链\n",
    "add_routes(app, chain, path=\"/qa\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 5.3 集成其他 FastAPI 组件\n",
    "\n",
    "LangServe 应用可以与其他 FastAPI 组件无缝集成：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "# 添加 CORS 支持\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# 添加自定义中间件\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.time() - start_time\n",
    "    response.headers[\"X-Process-Time\"] = str(process_time)\n",
    "    return response\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 六、部署与生产化\n",
    "\n",
    "### 6.1 本地开发\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 6.2 生产部署\n",
    "\n",
    "推荐使用 Gunicorn + Uvicorn 进行生产部署：\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "pip install gunicorn\n",
    "gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app -b 0.0.0.0:8000\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 6.3 Docker 部署\n",
    "\n",
    "创建 Dockerfile：\n",
    "\n",
    "dockerfile\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "构建并运行容器：\n",
    "\n",
    "bash\n",
    "\n",
    "```bash\n",
    "docker build -t langserve-app .\n",
    "docker run -p 8000:80 langserve-app\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 七、常见问题与解决方案\n",
    "\n",
    "### 7.1 跨域请求问题\n",
    "\n",
    "如果你在前端应用中调用 LangServe API 时遇到跨域问题，添加 CORS 中间件：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],  # 替换为你的前端域名\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 7.2 调试技巧\n",
    "\n",
    "1. 使用 `--reload` 选项在开发期间启用自动重载\n",
    "2. 查看 Swagger UI 文档以了解 API 结构\n",
    "3. 使用日志记录调试信息：\n",
    "\n",
    "python运行\n",
    "\n",
    "```python\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 在链中添加日志记录\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.verbose = True  # 启用详细日志\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 7.3 性能优化\n",
    "\n",
    "1. 使用异步 LLM 提供者（如 OpenAI 的异步 API）\n",
    "2. 增加工作进程数量：`gunicorn -w 8 -k uvicorn.workers.UvicornWorker main:app`\n",
    "3. 考虑使用负载均衡器（如 Nginx）进行水平扩展\n",
    "\n",
    "## 八、总结\n",
    "\n",
    "LangServe 提供了一种简单而强大的方式来部署 LangChain 应用，使开发者能够专注于构建智能应用，而不必担心复杂的服务端实现。通过自动生成 API 端点和文档，LangServe 大大加速了从开发到生产的过程，是构建语言模型驱动服务的理想选择。\n",
    "\n",
    "通过本指南，你应该能够：\n",
    "\n",
    "- 理解 LangServe 的基本概念和优势\n",
    "- 快速部署简单和复杂的 LangChain 链\n",
    "- 使用高级功能如认证和自定义路由\n",
    "- 将服务部署到生产环境\n",
    "\n",
    "如需更详细的文档，请参考 [LangServe 官方文档](https://langserve.readthedocs.io/)。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
